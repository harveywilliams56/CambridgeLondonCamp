{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a442e195",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d997d",
   "metadata": {},
   "source": [
    "## Generic AI\n",
    "\n",
    "AI is just a function, with lots of parameters we can change, which takes an input and produces an output.\n",
    "\n",
    "<img alt=\"\" width=\"500\" caption=\"Machine Learning Schematic\" src=\"../data/lesson_images/ML_Schematic.png\" id=\"ML_Schematic\"/>\n",
    "\n",
    "When we train an AI model, we measure the distance between the AI model output and the correct output - this is called the **loss**. We then adjust the parameters to make the model output a little closer to the correct ouput.\n",
    "\n",
    "<img alt=\"\" width=\"500\" caption=\"Training Machine Learning Schematic\" src=\"../data/lesson_images/ML_Training_Schematic.png\" id=\"ML_Training_Schematic\"/>\n",
    "\n",
    "Remember the loss from lesson 1?\n",
    "\n",
    "<img alt=\"\" width=\"500\" caption=\"The loss from our height-shoe-size AI\" src=\"../data/lesson_images/loss.png\" id=\"loss\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ded49",
   "metadata": {},
   "source": [
    "## Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1f9e3",
   "metadata": {},
   "source": [
    "<img alt=\"Natural and artificial neurons\" width=\"500\" caption=\"Natural and artificial neurons\" src=\"../data/lesson_images/chapter7_neuron.png\" id=\"neuron\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02734ec",
   "metadata": {},
   "source": [
    "Inspired by real neurones - multiple incoming signals arrive at different dendrites, with different sensitivity (**weight**). If they add together in the right combination, the neurone fires.\n",
    "\n",
    "Let's build a simple neural net - the Rosenblatt perceptron - to read handwritten numbers. It will have 10 neurones, one for each number from 0...9 that we want to detect. The right neurone should turn on for the right number. \n",
    "\n",
    "Our Perceptron will \"see\" images of numbers (from the MNIST public training set) which are 28x28 = 784 pixels. So each neurone will have 784 dendrites - one for each pixel in the image - and each dendrite will have its own sensitivity to each pixel, called the **weight**. \n",
    "\n",
    "Each neurone will also have a total sensitivity which decides how easily it is turned on by the pixels. This is called the **bias** (which has more than one meaning in AI - ask me about it!).\n",
    "\n",
    "In total our Perceptron model will have 784x10 = 7840 **weights** and 1x10 = 10 **biases**. So it has 7850 total parameters it must learn.\n",
    "\n",
    "Today we will only make our perceptron learn how to recognise two numbers - 3 and 7 - using two neurones. This means our model only needs 784x2 = 1568 weights and 1x2 = 2 biases for a total of 1570 parameters. This will be approximately 5 times faster to train than learning all 10 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b23afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE) # Download the MNIST training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change this code\n",
    "\n",
    "test_image_path = (path/'valid'/'3').ls().sorted()[0] # Try chanigng the 3 to another number and see what happens\n",
    "test_image = Image.open(test_image_path)\n",
    "test_image_tensor = tensor(test_image).view(-1, 28*28).float()/255\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb19797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS \n",
    "\n",
    "# This code opens up all of our images and puts them into two big lists\n",
    "# We use a library called 'torch' becuase it does it in a way that allows\n",
    "# ..the computer to make very fast calculations\n",
    "\n",
    "neurone_labels = ['3', '7'] # We are only going to train the model to recognise 3s and 7s\n",
    "\n",
    "\n",
    "stacked_threes = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'3').ls()]).float()/255 \n",
    "\n",
    "stacked_sevens = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'7').ls()]).float()/255 \n",
    "\n",
    "\n",
    "\n",
    "stacked_sevens.shape, stacked_threes.shape # This shows the shape of our lists\n",
    "# Our lists have around 6000 images, each of which is 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "\n",
    "# Here we combine our two lists into one big list\n",
    "# We also take each 28x28 square image and turn it into a long list of 784 pixels\n",
    "\n",
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "train_x.shape\n",
    "\n",
    "# Our list shape is (60000, 784) - 12000 total images squished into flat lists of 784 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same as above but only with three and seven so that training is faster\n",
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "train_y = tensor([[1.0,0.0]]*len(stacked_threes) + [[0.0, 1.0]]*len(stacked_sevens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerPerceptron():\n",
    "\n",
    "    def __init__(self, std=1.0):\n",
    "        self.weights = (torch.randn(28*28, 2)*std).requires_grad_() # This \n",
    "        self.biases = (torch.randn(2)*std).requires_grad_() # This is the bias for the neurone\n",
    "\n",
    "    def predict(self, x):\n",
    "        # return F.softmax(((x @ self.weights) + self.biases).sigmoid(), dim=1)\n",
    "        return F.softmax((x @ self.weights) + self.biases, dim=1) # This is the output of our model \n",
    "\n",
    "\n",
    "    def loss(self, input, training_output):\n",
    "        return F.mse_loss(self.predict(input), training_output)\n",
    "    \n",
    "    def train_one_step(self, input_batch, training_output_batch):\n",
    "\n",
    "        x = input_batch\n",
    "        y = training_output_batch\n",
    "\n",
    "        loss = self.loss(x, y)\n",
    "        loss.backward() # This calculates the gradients for our weights and biases\n",
    "        with torch.no_grad(): # This means we are not calculating gradients for the next step\n",
    "            self.weights -= self.weights.grad * 0.1 # This is the learning rate\n",
    "            self.biases -= self.biases.grad * 0.1\n",
    "        self.weights.grad.zero_()\n",
    "        self.biases.grad.zero_()\n",
    "        return loss.item()\n",
    "    \n",
    "    def get_batch(self, input, training_output, batch_size=64):\n",
    "        idxs = torch.randint(0, len(input), (batch_size,))\n",
    "        # print(f\"Batch indices: {idxs}\")  # Debugging line to check indices\n",
    "        return input[idxs], training_output[idxs]\n",
    "    \n",
    "    def train(self, input, training_data, cycles, batch_size=64):\n",
    "        for i in range(cycles):\n",
    "            input_batch, training_output_batch = self.get_batch(input, training_data, batch_size)\n",
    "            loss = self.train_one_step(input_batch, training_output_batch)\n",
    "            correct_percentage = (self.predict(input_batch).argmax(dim=1) == training_output_batch.argmax(dim=1)).float().mean().item()\n",
    "            if i % 20 == 0:\n",
    "                print(f\"Cycle {i}, Loss: {loss:.4f}, Accuracy: {correct_percentage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41951c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = OneLayerPerceptron() # Create a model with 784 inputs and 2 outputs (for the two neurones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35bdde",
   "metadata": {},
   "source": [
    "**Remember the test number from earlier?** Let's see if our model can predict the right answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "\n",
    "# Remember the test image we opened earlier? Let's see if our model can predict what number it is!\n",
    "print(f'The model predicts that the number is: {neurone_labels[model.predict(test_image_tensor).argmax(dim=1).item()]}')\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceace345",
   "metadata": {},
   "source": [
    "**The prediction is completely wrong!** This is because we haven't *trained* the model yet... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_x, train_y, 1000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb1d0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d51d49",
   "metadata": {},
   "source": [
    "**If the final accuracy above is less than 92% then you should run the training again**\n",
    "\n",
    "**If running the training again doesn't result in rapid improvement towards 90% then reset the model!**\n",
    "\n",
    "Now let's see what the AI model thinks our test number is...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "prediction = model.predict(test_image_tensor)\n",
    "\n",
    "print(f'Prediction: {neurone_labels[prediction.argmax(dim=1).item()]}, Confidence: {prediction.max().item():.2f}')\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beeb9ee",
   "metadata": {},
   "source": [
    "**Draw a number 3 or 7 yourself, take a photo and upload it. Use the below code to see what our trained AI model thinks the number is.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac7045",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f551b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "\n",
    "# This is some helper code to load your photo\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_np(image_path, crop_box=None):\n",
    "    \"\"\"\n",
    "    Load an image, optionally crop, resize to 28×28 grayscale,\n",
    "    normalize to [0,1], flatten to (784,), and display the image.\n",
    "    \"\"\"\n",
    "    # Load and optional crop\n",
    "    img = Image.open(image_path)\n",
    "    if crop_box:\n",
    "        img = img.crop(crop_box)\n",
    "    # Grayscale & resize\n",
    "    img = img.convert('L').resize((28,28), resample=Image.LANCZOS)\n",
    "    # Normalize to [0,1]\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0  # shape (28,28)\n",
    "    arr = 1.0 - arr  # Invert the image (white becomes black and vice versa)\n",
    "\n",
    "    # Display the processed image\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(arr, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title('Processed 28×28 Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Flatten to (784,)\n",
    "    flat = arr.ravel()\n",
    "    return flat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211f6a7",
   "metadata": {},
   "source": [
    "# This is how you open your photo!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Find the first image file in ../data/\n",
    "data_dir = \"../data/\"\n",
    "image_path = None\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\", \".tiff\", \".webp\")):\n",
    "        image_path = os.path.join(data_dir, filename)\n",
    "        break\n",
    "\n",
    "# Make sure an image was found\n",
    "if image_path is None:\n",
    "    raise FileNotFoundError(\"No image file found in ../data/\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "flat_vector = load_and_preprocess_np(image_path)\n",
    "\n",
    "# Run prediction\n",
    "input_tensor = tensor(flat_vector).view(-1, 28*28)\n",
    "prediction = model.predict(input_tensor)\n",
    "predicted_number = prediction.argmax(dim=1).item()\n",
    "confidence = prediction.max().item()\n",
    "\n",
    "# Print result\n",
    "print(f'The trained AI model predicts that the number is: {predicted_number}, '\n",
    "      f'with confidence {confidence:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this code to open your own photo\n",
    "\n",
    "# Save your photo in the lesson_images folder and give it a name like \"my_number.jpg\"\n",
    "\n",
    "# Change phone_crop.jpg to the name of your photo\n",
    "\n",
    "flat_vector = load_and_preprocess_np('../data/lesson_images/phone_crop.jpg') \n",
    "\n",
    "print(f'The trained AI model predicts that the number is: {neurone_labels[model.predict(tensor(flat_vector).view(-1, 28*28)).argmax(dim=1).item()]}, \\\n",
    "with confidence {model.predict(tensor(flat_vector).view(-1, 28*28)).max().item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d62d5",
   "metadata": {},
   "source": [
    "## Neural Nets and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713bf98",
   "metadata": {},
   "source": [
    "The single layer perceptron we made above only has one layer of neurones. However, a real brain (and real **deep** learning models) use many many layers, with the axon terminals from one layer connecting to the dendrites of the next layer.\n",
    "\n",
    "<img alt=\"Single layer perceptron diagram\" width=\"500\" caption=\"Single layer perceptron\" src=\"../data/lesson_images/rosenblattPerceptron.png\" id=\"rosenblattPerceptron\"/>\n",
    "\n",
    "<img alt=\"Deep perceptron diagram\" width=\"500\" caption=\"Deep perceptron\" src=\"../data/lesson_images/deepPerceptron.png\" id=\"deepPerceptron\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c90e7",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8808e",
   "metadata": {},
   "source": [
    "If we plot our single layer perceptron weights as an image, we see they look a lot like the numbers we're trying to detect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "\n",
    "# This is just a helper function to make our plot have higher contrast\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def increase_contrast(img: np.ndarray, low_percentile: float = 2, high_percentile: float = 98) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This is just a function to increase the contrast of a single-channel image\n",
    "    \"\"\"\n",
    "    # Compute percentile cutoffs\n",
    "    low, high = np.percentile(img, (low_percentile, high_percentile))\n",
    "    # Stretch and clip\n",
    "    stretched = (img - low) / (high - low)\n",
    "    stretched = np.clip(stretched, 0.0, 1.0)\n",
    "    return stretched\n",
    "\n",
    "# Example usage:\n",
    "# Assuming weight_imgs[0] is your (28,28) weight image:\n",
    "# enhanced_img = increase_contrast(weight_imgs[0], low_percentile=1, high_percentile=99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize before/after:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Grab the learned weights \n",
    "W = model.weights.detach().cpu().numpy()\n",
    "\n",
    "# 2) Turn each neuron’s weight‐vector into a 28×28 image\n",
    "weight_imgs = [W[:, i].reshape(28, 28) for i in range(W.shape[1])]\n",
    "original = weight_imgs[0] \n",
    "enhanced = increase_contrast(original, 20, 80)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax1.imshow(original, cmap='gray')\n",
    "ax1.set_title('Original Weights for 3')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(enhanced, cmap='gray')\n",
    "ax2.set_title('Extra Contrast Weights for 3')\n",
    "ax2.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40596471",
   "metadata": {},
   "source": [
    "**Your version of the weights does not look like the number 3!** There is a complicated reason why - ask me!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fa255",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/chapter9_conv_basic.png\" id=\"basic_conv\" caption=\"Applying a kernel to one location\" alt=\"Applying a kernel to one location\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce6fcf",
   "metadata": {},
   "source": [
    "<img alt=\"A 4×4 kernel with 5×5 input and 2 pixels of padding\" width=\"700\" caption=\"A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"four_by_five_conv\" src=\"../data/lesson_images/att_00029.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7fade",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/convolutionExample.png\" id=\"convolutionExample\" caption=\"Applying a kernel to a whole image\" alt=\"Example of a convolution working\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82fb72",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/layer1.png\" alt=\"Activations of the first layer of a CNN\" width=\"300\" caption=\"Activations of the first layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer1\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533663e",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/layer2.png\" alt=\"Activations of the second layer of a CNN\" width=\"800\" caption=\"Activations of the second layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486be74",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/chapter2_layer3.PNG\" alt=\"Activations of the third layer of a CNN\" width=\"800\" caption=\"Activations of the third layer of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c4882",
   "metadata": {},
   "source": [
    "<img src=\"../data/lesson_images/chapter2_layer4and5.PNG\" alt=\"Activations of layers 4 and 5 of a CNN\" width=\"800\" caption=\"Activations of layers 4 and 5 of a CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\" id=\"img_layer4\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
